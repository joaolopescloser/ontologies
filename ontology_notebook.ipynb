{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from gensim.utils import lemmatize\n",
    "import collections\n",
    "import operator\n",
    "import json\n",
    "import utilsOntology as uo\n",
    "from utilsOntology import stop_words, df, labels_array\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import requests\n",
    "import numpy as np\n",
    "import warnings\n",
    "import codecs\n",
    "from itertools import compress, product\n",
    "import bottle\n",
    "from bottle import route, run, template, response, static_file, request, post, get, put, delete\n",
    "import os\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def Combinations(items):\n",
    "    return (list( list(compress(items,mask)) for mask in product(*[[0,1]]*len(items)))[::-1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def DfWord(word_to_id = None, word = None):\n",
    "    try:\n",
    "        return df[word_to_id[word]]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetAntonyms(word_list=list()):\n",
    "    antonyms_ = []\n",
    "    for word in word_list:\n",
    "        for syn in wordnet.synsets(word): \n",
    "            for l in syn.lemmas(): \n",
    "                if l.antonyms(): \n",
    "                    antonyms_.append(l.antonyms()[0].name()) \n",
    "    if len(antonyms_) == 0:\n",
    "        antonyms_ = [\"thing\"]\n",
    "    \n",
    "    return list(set(antonyms_))\n",
    "\n",
    "\n",
    "def WordArithmetic(nouns=dict(), num_results=10):\n",
    "    minus_words_ = nouns[\"negative\"]\n",
    "    plus_words_ = nouns[\"positive\"]\n",
    "    '''Returns a word string that is the result of the vector arithmetic'''\n",
    "    word_to_id_, id_to_word_ = uo.get_label_dictionaries(labels_array)\n",
    "    \n",
    "    plus_vecs_ = []\n",
    "    minus_vecs_ = []\n",
    "    plus_words_selected_ = []\n",
    "    #\n",
    "    for plus_word in plus_words_:\n",
    "        df_word_ = DfWord(word_to_id=word_to_id_, word=plus_word)\n",
    "        if len(df_word_)>0:\n",
    "            plus_vecs_.append(df_word_)\n",
    "            plus_words_selected_.append(plus_word)\n",
    "    \n",
    "    for minus_word in minus_words_:\n",
    "        df_word_ = DfWord(word_to_id=word_to_id_, word=minus_word)\n",
    "        if len(df_word_)>0:\n",
    "            minus_vecs_.append(df_word_)\n",
    "            \n",
    "    # Get start word\n",
    "    if len(plus_vecs_)==0:\n",
    "        antonyms_ = GetAntonyms(word_list=minus_words_)\n",
    "        for antonym in antonyms_:\n",
    "            antonym_aux_ = DfWord(word_to_id=word_to_id_, word=antonym)\n",
    "            if antonym_aux_ != []:\n",
    "                plus_vecs_.append(antonym_aux_)\n",
    "    \n",
    "        plus_words_selected_ += antonyms_\n",
    "    \n",
    "    start_word_ = random.choice(plus_words_selected_)\n",
    "    if start_word_ in plus_words_:\n",
    "        plus_words_.remove(start_word_)\n",
    "    \n",
    "    if len(minus_vecs_)==0:\n",
    "        antonyms_ = GetAntonyms(word_list=plus_words_)\n",
    "        for antonym in antonyms_:\n",
    "            antonym_aux_ = DfWord(word_to_id=word_to_id_,word=antonym)\n",
    "            if antonym_aux_ != []:\n",
    "                minus_vecs_.append(antonym_aux_)\n",
    "                minus_words_.append(antonym)\n",
    "    if start_word_ in word_to_id_:\n",
    "        result_ = df[word_to_id_[start_word_]]\n",
    "    else:\n",
    "        result_ = df[word_to_id_[\"thing\"]]\n",
    "    \n",
    "    if minus_vecs_:\n",
    "        for i, vec in enumerate(minus_vecs_):\n",
    "            result_ = result_ - vec\n",
    "\n",
    "    if plus_vecs_:\n",
    "        for i, vec in enumerate(plus_vecs_):\n",
    "            result_ = result_ + vec\n",
    "\n",
    "    words_ = [start_word_] + minus_words_ + plus_words_\n",
    "    return FindNearest(words=words_, vec=result_, id_to_word=id_to_word_, num_results=num_results)\n",
    "\n",
    "\n",
    "def FindNearest(words=list(), vec=list(), id_to_word=dict(), num_results=10):\n",
    "    minim_ = [] # min, index\n",
    "    english_vocab_ = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    for i, v in enumerate(df):\n",
    "        # skip the base word, its usually the closest\n",
    "        if id_to_word[i] in words:\n",
    "            continue\n",
    "        dist_ = cosine(vec, v)\n",
    "        minim_.append((dist_, i))\n",
    "    minim_ = sorted(minim_, key=lambda v: v[0])\n",
    "    # return list of (word, cosine distance) tuples\n",
    "    word_list_ = [id_to_word[minim_[i][1]] for i in range(num_results)]\n",
    "    #word_list__ = [w for w in word_list_ if w not in stop_words] \n",
    "    word_list__ = []\n",
    "    for w in word_list_:\n",
    "        if w not in stop_words and w.lower() in english_vocab_:\n",
    "            word_list__.append(w)\n",
    "            \n",
    "    return word_list__[:num_results]\n",
    "\n",
    "\n",
    "def hyper(synset=None):\n",
    "    return synset.hypernyms()\n",
    "\n",
    "def hypo(synset=None):\n",
    "    return synset.hyponyms()\n",
    "\n",
    "def partMero(synset=None):\n",
    "    return synset.part_meronyms()\n",
    "\n",
    "def substanceMero(synset=None):\n",
    "    return synset.substance_meronyms()\n",
    "\n",
    "def partHolo(synset=None):\n",
    "    return synset.part_holonyms()\n",
    "\n",
    "def substanceHolo(synset=None):\n",
    "    return synset.substance_holonyms()\n",
    "\n",
    "def memberHolo(synset=None):\n",
    "    return synset.substance_holonyms()\n",
    "\n",
    "def js(obj):\n",
    "    try:\n",
    "        print json.dumps(obj, indent=3)\n",
    "    except:\n",
    "        print obj\n",
    "\n",
    "\n",
    "def WordCompare2Definition(word=None, synsets=list()):\n",
    "    bag_of_synsets_ = []\n",
    "    for s in synsets:\n",
    "        \n",
    "        definition_ = s.definition()\n",
    "        definition_list_ = definition_.split()\n",
    "        try:\n",
    "            lesk_result_name_ = lesk(definition_list_,word, pos = 'n').name()\n",
    "            if \".n.\" in lesk_result_name_:\n",
    "                bag_of_synsets_.append(lesk_result_name_)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    bag_of_synsets_counter_ = collections.Counter(bag_of_synsets_)\n",
    "    bag_of_synsets_max_list_ = []\n",
    "    if len(bag_of_synsets_counter_) > 0:\n",
    "        bag_of_synsets_max_ = bag_of_synsets_counter_[max(bag_of_synsets_counter_.iteritems(), key=operator.itemgetter(1))[0]]\n",
    "    \n",
    "        for k, v in bag_of_synsets_counter_.items():\n",
    "            if v == bag_of_synsets_max_:\n",
    "                bag_of_synsets_max_list_.append(k)\n",
    "       \n",
    "    return bag_of_synsets_max_list_\n",
    "\n",
    "\n",
    "def GetSynsetPairs(synset_list=list()):\n",
    "    # synset_list = [{\"word\":w,\"synset\":synset_of_word,\"vote\":1}]\n",
    "    aux_ = []\n",
    "    considered_pairs_dict_ = {}\n",
    "\n",
    "    for it_ in synset_list:\n",
    "        synset_for1_ = it_[\"synset\"]\n",
    "                \n",
    "        for it__ in synset_list:\n",
    "            synset_for2_ = it__[\"synset\"]\n",
    "            if synset_for1_ != synset_for2_ and synset_for1_+synset_for2_ not in aux_ and synset_for2_+synset_for1_ not in aux_:\n",
    "                # S(AB) = S(BA)\n",
    "                if considered_pairs_dict_.has_key(synset_for1_) == False:\n",
    "                    considered_pairs_dict_[synset_for1_] = []\n",
    "                aux_ += [synset_for1_ + synset_for2_, synset_for2_ + synset_for1_]\n",
    "                considered_pairs_dict_[synset_for1_].append([it_, it__])\n",
    "\n",
    "    return considered_pairs_dict_\n",
    "\n",
    "\n",
    "def SemanticSimilarity(synset_pairs=list(), distance_root_max=5):\n",
    "    \n",
    "    synset_origin_ =  wn.synset(synset_pairs[0][\"synset\"])\n",
    "    orig_vote_ = synset_pairs[0][\"vote\"]\n",
    "    dest_vote_ = synset_pairs[1][\"vote\"]   \n",
    "    orig_word_ = synset_pairs[0][\"word\"]\n",
    "    synset_destination_ = wn.synset(synset_pairs[1][\"synset\"])\n",
    "    \n",
    "    if dest_vote_ < orig_vote_:\n",
    "        synset_origin_ = wn.synset(synset_pairs[1][\"synset\"])\n",
    "        orig_vote_ = synset_pairs[1][\"vote\"]\n",
    "        dest_vote_ = synset_pairs[0][\"vote\"]   \n",
    "        orig_word_ = synset_pairs[1][\"word\"]\n",
    "        synset_destination_ = wn.synset(synset_pairs[0][\"synset\"])\n",
    "    \n",
    "\n",
    "    if orig_vote_ == dest_vote_ and dest_vote_ == 1:\n",
    "        obj_vote_ = 1 # get closer\n",
    "    else:\n",
    "        obj_vote_ = 0 # get far away\n",
    "\n",
    "    dist_dict_ = {'hyper': hyper(synset=synset_origin_),\n",
    "                  'hypo': hypo(synset=synset_origin_),\n",
    "                  'partmero': partMero(synset=synset_origin_),\n",
    "                  'substancemero': substanceMero(synset=synset_origin_),\n",
    "                  'partholo' : partHolo(synset=synset_origin_),\n",
    "                  'substanceholo': substanceHolo(synset=synset_origin_),\n",
    "                  'memberHolo': memberHolo(synset=synset_origin_)\n",
    "                 }\n",
    "    similarity_original_ = synset_origin_.path_similarity(synset_destination_)\n",
    "    semantic_relationship_ = None\n",
    "\n",
    "    for k,v in dist_dict_.items():\n",
    "        for synset in v:\n",
    "            if \".n.\" in synset.name():\n",
    "                similarity_ = synset.path_similarity(synset_destination_)\n",
    "                distance_root_ = min([len(path) for path in synset.hypernym_paths()])\n",
    "\n",
    "                if obj_vote_ == 1 and similarity_ > similarity_original_ and distance_root_ > distance_root_max:\n",
    "                    similarity_original_ = similarity_\n",
    "                    semantic_relationship_ = k\n",
    "                    hyper_synset_ = synset\n",
    "\n",
    "                if obj_vote_ == 0 and similarity_ < similarity_original_ and distance_root_ > distance_root_max:\n",
    "                    similarity_original_ = similarity_\n",
    "                    semantic_relationship_ = k\n",
    "                    hyper_synset_ = synset\n",
    "    try:\n",
    "        candidate_ = hyper_synset_.name()\n",
    "    except:\n",
    "        candidate_ = synset_origin_.name()\n",
    "\n",
    "    result_dict_ = {\n",
    "                    'semantic_relationship': semantic_relationship_,\n",
    "                    'candidate': candidate_,\n",
    "                    'semantic_similarity': similarity_original_,\n",
    "                    'synsets_pair': synset_pairs,\n",
    "                    'new_synset': {\"synset\":candidate_, \"vote\": obj_vote_,\"word\":orig_word_}\n",
    "                    }\n",
    "\n",
    "    return result_dict_\n",
    "\n",
    "\n",
    "def Crawler(considered_pairs_dict=dict(),tier_dict=dict(),synset_list=None):\n",
    "    len_considered_pairs_dict_ = len(considered_pairs_dict)\n",
    "    key_considered_pairs_dict_ = None\n",
    "    if not tier_dict:\n",
    "        tier_dict[len_considered_pairs_dict_+1] = synset_list\n",
    "\n",
    "    if len_considered_pairs_dict_ > 1:\n",
    "        synset_list_mod_ = []\n",
    "\n",
    "        for synset in considered_pairs_dict:\n",
    "            result_max_min_semantic_ = None\n",
    "            selected_pair_ = None\n",
    "            new_synset_ = None\n",
    "\n",
    "            for pair in considered_pairs_dict[synset]:\n",
    "                result_dict_ = SemanticSimilarity(synset_pairs=pair)\n",
    "                result_semantic_ = result_dict_['semantic_similarity']\n",
    "                if result_dict_['new_synset'][\"vote\"] == 1:\n",
    "                    if not result_max_min_semantic_ or result_semantic_ > result_max_min_semantic_:\n",
    "                        result_max_min_semantic_ = result_semantic_\n",
    "                        selected_pair_ = pair\n",
    "                        new_synset_ = result_dict_['new_synset']\n",
    "                else: # result_obj_func = 0\n",
    "                    if not result_max_min_semantic_ or result_semantic_ < result_max_min_semantic_:\n",
    "                        result_max_min_semantic_ = result_semantic_\n",
    "                        selected_pair_ = pair\n",
    "                        new_synset_ = result_dict_['new_synset']\n",
    "\n",
    "            synset_list_mod_.append(new_synset_)\n",
    "        \n",
    "        synset_list_mod_unique_ = [dict(t) for t in {tuple(d.items()) for d in synset_list_mod_}]\n",
    "        considered_pairs_dict_mod_ = GetSynsetPairs(synset_list=synset_list_mod_unique_)\n",
    "        key_considered_pairs_dict_,tier_dict = Crawler(considered_pairs_dict=considered_pairs_dict_mod_,tier_dict=tier_dict)\n",
    "        tier_dict[len_considered_pairs_dict_] = synset_list_mod_unique_\n",
    "\n",
    "    elif len_considered_pairs_dict_ == 1:\n",
    "        synset_list_mod_ = considered_pairs_dict[considered_pairs_dict.keys()[0]][0]\n",
    "        compare_result_ = SemanticSimilarity(synset_list_mod_)\n",
    "        key_considered_pairs_dict_ = compare_result_[\"candidate\"]\n",
    "        tier_dict[len_considered_pairs_dict_]=[compare_result_[\"new_synset\"]]\n",
    "\n",
    "    return key_considered_pairs_dict_,tier_dict\n",
    "\n",
    "\n",
    "def GetNounsFromDefinition(definition=str()):\n",
    "    nouns_ = []\n",
    "    lemma_ = lemmatize(definition)\n",
    "    for word in lemma_:\n",
    "        word_pos_ = word.split('/')\n",
    "        if word_pos_[1][0] in ['N', 'R', 'J']:\n",
    "            nouns_.append(word_pos_[0])\n",
    "\n",
    "    return nouns_\n",
    "\n",
    "\n",
    "def Synset2Definition(synset_dict=dict()):\n",
    "    # synset_dict={\"word\":w,\"synset\":synset_of_word,\"vote\":?}\n",
    "    definition_ = wn.synset(synset_dict[\"synset\"]).definition()\n",
    "    definition_nouns_ = GetNounsFromDefinition(definition_)\n",
    "    \n",
    "    synset_dict[\"definition\"] = definition_\n",
    "    synset_dict[\"definition_nouns\"] = definition_nouns_\n",
    "\n",
    "    return synset_dict\n",
    "\n",
    "\n",
    "def GetGloveDistance(synset_list=list()):\n",
    "    \n",
    "    # synset_list = [{\"word\":w,\"synset\":synset_of_word,\"vote\":?,\"definition\":definition,\"nouns\":[noun]}]\n",
    "    # > king - man + woman\n",
    "    # queen                0.31\n",
    "    # monarch              0.44\n",
    "    # throne               0.44\n",
    "    \n",
    "    new_words_ = []\n",
    "    nouns_positive_ = []\n",
    "    nouns_negative_ = []\n",
    "    for synset in synset_list:\n",
    "        vote_= synset[\"vote\"]\n",
    "        nouns_ = synset[\"definition_nouns\"]\n",
    "        if vote_:\n",
    "            nouns_positive_ += nouns_\n",
    "        else:\n",
    "            nouns_negative_ += nouns_\n",
    "    nouns_ = {\"positive\":nouns_positive_, \"negative\":nouns_negative_}\n",
    "    new_words_ = WordArithmetic(nouns=nouns_, num_results=10)\n",
    "\n",
    "    return new_words_\n",
    "\n",
    "\n",
    "def GetTier(synset_crawler_tiers=list(),tier=3):\n",
    "    synset_crawler_tiers_len_ = len(synset_crawler_tiers[1])\n",
    "    if synset_crawler_tiers_len_<tier:\n",
    "        tier = synset_crawler_tiers_len_\n",
    "    try:\n",
    "        return synset_crawler_tiers[1][tier]\n",
    "    except:\n",
    "        return synset_crawler_tiers[1][1]\n",
    "\n",
    "\n",
    "\n",
    "def GetFlickrFeedback(words=list(),key=\"cc6129bf7aa20c0c7aad87c0843f46b5\",words_iteractor=0):\n",
    "    \n",
    "    words_ = words[words_iteractor]\n",
    "    words_iteractor += 1\n",
    "    \n",
    "    if len(words_) == 0:\n",
    "        words_ = [\"end\"]\n",
    "    \n",
    "\n",
    "    english_vocab_ = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    lemmatizer_ = WordNetLemmatizer()\n",
    "    tags_string_ = \"\"\n",
    "    \n",
    "    # Create part of the URL\n",
    "    for t in words_:\n",
    "        tags_string_ += t+\",\"\n",
    "    \n",
    "    tags_string_ = tags_string_[:-1]\n",
    "    \n",
    "    try:\n",
    "        r_ = requests.get('https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key='+key+'&tags='+tags_string_+'&tag_mode=any&format=json&nojsoncallback=1')\n",
    "        \n",
    "        photo_ = random.choice(r_.json()[\"photos\"][\"photo\"])\n",
    "        photo_id_ = photo_[\"id\"]\n",
    "        photo_title_ = photo_[\"title\"]\n",
    "        photo_owner_id_ = photo_[\"owner\"]\n",
    "        \n",
    "        r_ = requests.get('https://api.flickr.com/services/rest/?method=flickr.tags.getListPhoto&api_key='+key+'&photo_id='+str(photo_id_)+'&format=json&nojsoncallback=1')\n",
    "        \n",
    "        photo_tags_ = r_.json()[\"photo\"][\"tags\"][\"tag\"]\n",
    "        photo_owner_name_ = photo_tags_[0][\"authorname\"]\n",
    "        photo_tags_list_ = []\n",
    "        \n",
    "        for t in photo_tags_:\n",
    "            photo_tags_list_tmp_ = t[\"raw\"].split()\n",
    "           \n",
    "            for l in photo_tags_list_tmp_:\n",
    "                if l.lower() in english_vocab_:\n",
    "                    photo_tags_list_.append(lemmatizer_.lemmatize(l.lower()))\n",
    "            \n",
    "        r_ = requests.get('https://api.flickr.com/services/rest/?method=flickr.photos.getSizes&api_key='+key+'&photo_id='+str(photo_id_)+'&format=json&nojsoncallback=1')\n",
    "        \n",
    "        photo_sizes_ = r_.json()[\"sizes\"][\"size\"]\n",
    "        for t in photo_sizes_:\n",
    "            if t[\"label\"] == \"Large\":\n",
    "                photo_url_ = t[\"source\"]\n",
    "                break\n",
    "            \n",
    "        flickr_dict_ = {\"id\":photo_id_,\"title\":photo_title_,\"owner\":photo_owner_name_,\n",
    "                             \"tags\":photo_tags_list_,\"url\":photo_url_}\n",
    "    \n",
    "    except:\n",
    "        flickr_dict_ = GetFlickrFeedback(words=words,key=\"cc6129bf7aa20c0c7aad87c0843f46b5\",words_iteractor=words_iteractor)\n",
    "\n",
    "    return flickr_dict_\n",
    "\n",
    "\n",
    "def ReceiveFeedback(feedback=dict()):\n",
    "    # input = {\"synsets\":[],\"flickr\":{\"url\":url,\"words\":[],\"vote\":?}}\n",
    "    feedback_flickr_ = {\"words\":feedback[\"flickr\"][\"tags\"],\"vote\":feedback[\"flickr\"][\"vote\"]}\n",
    "    synsets_past_positive_ = [wn.synset(f) for f in feedback[\"synsets\"]]\n",
    "    synsets_present_negative_ = []\n",
    "    synset_list_ = []\n",
    "    synset_list2keep_ = []\n",
    "\n",
    "    print '\\033[1;32;40mGetting synsets...',\n",
    "\n",
    "    for word in feedback_flickr_[\"words\"]:\n",
    "        try:\n",
    "            word_synsets_ = wn.synsets(word, pos='n')\n",
    "        except:\n",
    "            word_synsets_ = []\n",
    "        if feedback_flickr_[\"vote\"]:\n",
    "            synsets_past_positive_ += word_synsets_\n",
    "        else:\n",
    "            synsets_present_negative_ += word_synsets_\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "    print '\\033[1;32;40mConsidered positive synsets:'\n",
    "    for it_ in synsets_past_positive_:\n",
    "        print it_,\n",
    "    print ' '\n",
    "    print '\\033[1;32;40mConsidered negative synsets:'\n",
    "    for it_ in synsets_present_negative_:\n",
    "        print it_,\n",
    "    print ' '\n",
    "\n",
    "\n",
    "    print '\\033[1;32;40mUpdating polarity of synsets...',\n",
    "    # Save positive synsets after deleting negatives\n",
    "    synsets_past_positive_ = list((set(synsets_past_positive_))-set(synsets_present_negative_))\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "\n",
    "    print '\\033[1;32;40mApllying word desambiguation algorithm...',\n",
    "    for word in feedback_flickr_[\"words\"]:\n",
    "        synset_of_word_ = WordCompare2Definition(word=word,synsets=synsets_past_positive_)\n",
    "        # FIRST COME THE NEGATIVES\n",
    "        for synset in synset_of_word_:\n",
    "            if feedback_flickr_[\"vote\"] == 0:\n",
    "                synset_dict_ = {\"word\":word,\"synset\":synset,\"vote\":feedback_flickr_[\"vote\"]}\n",
    "                synset_list_.append(synset_dict_)\n",
    "        for synset in synset_of_word_:\n",
    "            if feedback_flickr_[\"vote\"] == 1:\n",
    "                synset_dict_ = {\"word\":word,\"synset\":synset,\"vote\":feedback_flickr_[\"vote\"]}\n",
    "                synset_list_.append(synset_dict_)\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "\n",
    "\n",
    "    print '\\033[1;32;40mCreating synset pairs for polarity semantic analysis...',\n",
    "    considered_pairs_dict_ = GetSynsetPairs(synset_list=synset_list_)\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "    print \"\\033[1;32;40mApllying Semantic Crawlers on WordNet graph...\",\n",
    "    synset_crawler_tiers_ = Crawler(considered_pairs_dict=considered_pairs_dict_,synset_list=synset_list_)\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "    synset_crawler_tier_list_ = GetTier(synset_crawler_tiers=synset_crawler_tiers_)\n",
    "    print \"\\033[1;32;40mSelected synset candidates:\"\n",
    "    for k,s in enumerate(synset_list_):\n",
    "        synset_list_[k] = Synset2Definition(synset_dict=synset_list_[k])\n",
    "        synset_list2keep_.append(synset_list_[k][\"synset\"])\n",
    "    synset_list2keep_=list(set(synset_list2keep_))\n",
    "    for it_ in synset_list2keep_:\n",
    "        print it_,\n",
    "\n",
    "    print '\\033[1;32;40mTransforming synsets in words via Glove...',\n",
    "    new_words_list_ = GetGloveDistance(synset_list=synset_list_)\n",
    "    print '\\033[1;30;42m[ Done! ]\\033[1;32;40m'\n",
    "\n",
    "\n",
    "\n",
    "    if len(new_words_list_) < 1:\n",
    "        synset_list_just_synset_ = []\n",
    "        new_words_list_ = [l.split(\".\")[0] for l in list(set(synset_list2keep_))]\n",
    "   # return synset_list2keep_, new_words_list_\n",
    "    new_words_list_ = new_words_list_[:10]\n",
    "\n",
    "    print '\\033[1;32;40mConsidered words list:',\n",
    "    for it_ in new_words_list_:\n",
    "        print it_,\n",
    "    print ''\n",
    "\n",
    "    print '\\033[1;32;40mCalling Flickr API...'\n",
    "    words_flickr_ = Combinations(new_words_list_)\n",
    "\n",
    "    flickr_dict_ = GetFlickrFeedback(words=words_flickr_)\n",
    "\n",
    "    output_ = {\"synsets\":synset_list2keep_,\"flickr\":flickr_dict_}\n",
    "\n",
    "    return output_\n",
    "\n",
    "\n",
    "def ReceiveFirstWords(words=list()):\n",
    "    feedback_ = {\"synsets\":[],\"flickr\":{\"url\":None,\"tags\":words,\"vote\":1}}\n",
    "\n",
    "    return ReceiveFeedback(feedback=feedback_)\n",
    "\n",
    "@route('/static/<path:path>')\n",
    "def static(path):\n",
    "    return static_file(path, os.getcwd()+'/static')\n",
    "\n",
    "@route('/words/<words>')\n",
    "def GetWords(words):\n",
    "    words_list_ = words.split('+')\n",
    "    return ReceiveFirstWords(words=words_list_)\n",
    "\n",
    "@post('/vote/<vote>')\n",
    "def Vote(vote):\n",
    "    in_json_ = request.json\n",
    "    in_json_[\"flickr\"][\"vote\"] = vote\n",
    "    return ReceiveFeedback(feedback=in_json_)\n",
    "\n",
    "\n",
    "run(host = 'localhost', port = 8080, reloader = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
